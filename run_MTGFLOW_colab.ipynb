{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h4liRTfuzfb"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "U2b9P3P9hddF",
        "outputId": "2bcca36f-a61e-4b64-b56d-0946543e8ee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.5)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.12.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: tsfel in /usr/local/lib/python3.10/dist-packages (0.1.7)\n",
            "Requirement already satisfied: ipython>=7.4.0 in /usr/local/lib/python3.10/dist-packages (from tsfel) (7.34.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from tsfel) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /usr/local/lib/python3.10/dist-packages (from tsfel) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from tsfel) (1.3.2)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from tsfel) (1.13.1)\n",
            "Requirement already satisfied: setuptools>=47.1.1 in /usr/local/lib/python3.10/dist-packages (from tsfel) (71.0.4)\n",
            "Requirement already satisfied: statsmodels>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from tsfel) (0.14.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.4.0->tsfel) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.4.0->tsfel) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.4.0->tsfel) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.4.0->tsfel) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.4.0->tsfel) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.4.0->tsfel) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.4.0->tsfel) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=7.4.0->tsfel) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.4.0->tsfel) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.3->tsfel) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.3->tsfel) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->tsfel) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->tsfel) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12.0->tsfel) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12.0->tsfel) (24.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.4.0->tsfel) (0.8.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels>=0.12.0->tsfel) (1.16.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.4.0->tsfel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.4.0->tsfel) (0.2.13)\n",
            "Requirement already satisfied: datetime in /usr/local/lib/python3.10/dist-packages (5.5)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.10/dist-packages (from datetime) (6.4.post2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datetime) (2024.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface->datetime) (71.0.4)\n",
            "Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (1.26.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.7.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Collecting fastdtw\n",
            "  Using cached fastdtw-0.3.4.tar.gz (133 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fastdtw) (1.26.4)\n",
            "Building wheels for collected packages: fastdtw\n",
            "  Building wheel for fastdtw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastdtw: filename=fastdtw-0.3.4-cp310-cp310-linux_x86_64.whl size=512607 sha256=861472c808369c3e24059f77c049e1acacd825996404cea7de5c478880581a0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/c8/f7/c25448dab74c3acf4848bc25d513c736bb93910277e1528ef4\n",
            "Successfully built fastdtw\n",
            "Installing collected packages: fastdtw\n",
            "Successfully installed fastdtw-0.3.4\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (1.26.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (2.2.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (0.1.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!pip install scipy\n",
        "!pip install scikit-learn\n",
        "!pip install tsfel\n",
        "!pip install datetime\n",
        "!pip install pandas==1.5.3\n",
        "!pip install keras-tuner\n",
        "!pip install fastdtw\n",
        "!pip install tensorflow-probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSIs1Ja_kCGo"
      },
      "source": [
        "# Run experiments on Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YYIEb3skIlK",
        "outputId": "964f1d07-45b9-468e-f32f-abd9db399f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09u6MOfd9Yit",
        "outputId": "443e3db0-c924-40d4-f9c5-0d7d6cbc4219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')\n",
        "#save changes on Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MDc35_nBxNpz",
        "outputId": "bea236d8-7906-42a0-d330-21c1f2f016cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MTGFLOW experiments\n",
            "--- LOADING TRAINING SET FOR TUNING ---\n",
            "Loading data.\n",
            "Found 31 different actions.\n",
            "Loading data done.\n",
            "\n",
            "--- TUNING SCALER AND CORR PARAMETERS ---\n",
            "--- LOADING TRAINING SET ---\n",
            "Loading data.\n",
            "Found 31 different actions.\n",
            "Loading data done.\n",
            "\n",
            "--- PREPROCESSING TRAINING SPLIT 0 WITH OVERLAP 0.7 ---\n",
            "Loading data.\n",
            "Found 31 different actions.\n",
            "Loading data done.\n",
            "\n",
            "--- PREPROCESSING TRAINING SPLIT 1 WITH OVERLAP 0.7 ---\n",
            "Loading data.\n",
            "Found 31 different actions.\n",
            "Loading data done.\n",
            "\n",
            "--- PREPROCESSING TRAINING SPLIT 2 WITH OVERLAP 0.7 ---\n",
            "Loading data.\n",
            "Found 31 different actions.\n",
            "Loading data done.\n",
            "\n",
            "--- PREPROCESSING TRAINING SPLIT 3 WITH OVERLAP 0.7 ---\n",
            "window split train size:\n",
            "(3183, 100, 51)\n",
            "window action train size:\n",
            "3183\n",
            "--- LOADING TEST SET ---\n",
            "Loading data.\n",
            "Found 31 different actions.\n",
            "Loading data done.\n",
            "\n",
            "--- PREPROCESSING TEST SPLIT 0 ---\n",
            "Loading data.\n",
            "Found 31 different actions.\n",
            "Loading data done.\n",
            "\n",
            "--- PREPROCESSING TEST SPLIT 1 ---\n",
            "window split test size:\n",
            "(34077, 100, 51)\n",
            "window action test size:\n",
            "34077\n",
            "timestamps\n",
            "34077\n",
            "experiment 0/1\n",
            "config:\n",
            "{'experiment': 'MTGFLOW', 'lr': 5e-05, 'max_iterations': 5000, 'batch_size': 64, 'window_size': 100, 'output_path': '.', 'cpu': True, 'test': False, 'train': False, 'possible_window_size': [100], 'stride': 30, 'possible_num_features': [51], 'possible_hidden_size': [47], 'possible_n_blocks': [1], 'possible_lr': [5e-05], 'possible_dropout': [0.0], 'epochs': 50, 'tf': True, 'num_features': 51, 'hidden_size': 47, 'dropout': 0.0, 'n_blocks': 1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfede-bussolino\u001b[0m (\u001b[33mfede-bussolino-Politecnico di Torino\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/MLinApps-Proj-FP01/wandb/run-20240802_061116-x76whh4g</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/fede-bussolino-Politecnico%20di%20Torino/FP01/runs/x76whh4g' target=\"_blank\">MTGFLOW_20240802_061116</a></strong> to <a href='https://wandb.ai/fede-bussolino-Politecnico%20di%20Torino/FP01' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/fede-bussolino-Politecnico%20di%20Torino/FP01' target=\"_blank\">https://wandb.ai/fede-bussolino-Politecnico%20di%20Torino/FP01</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/fede-bussolino-Politecnico%20di%20Torino/FP01/runs/x76whh4g' target=\"_blank\">https://wandb.ai/fede-bussolino-Politecnico%20di%20Torino/FP01/runs/x76whh4g</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------START TRAINING--------------------------\n",
            "[5e-05, 4.5e-05, 4.05e-05, 3.6450000000000005e-05, 3.2805e-05, 2.9524500000000005e-05]\n",
            "epoch: 0/50\n",
            "Epoch 1 train loss: 1.9322, lr: 4.999999873689376e-05\n",
            "saved\n",
            "epoch: 1/50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/content/drive/MyDrive/MLinApps-Proj-FP01/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    192\u001b[0m                         filemode='a')\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/MLinApps-Proj-FP01/main.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------------------START TRAINING--------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----------------------------START TEST----------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_anomalies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollision_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollision_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/MLinApps-Proj-FP01/experiments/MTGFLOW.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, time)\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                     \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                     \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_minibatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    870\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/MLinApps-Proj-FP01')\n",
        "\n",
        "%run main.py --experiment MTGFLOW --cpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### publish by download and run on vscode"
      ],
      "metadata": {
        "id": "lQibEu095vC1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6yuI3SOov6N"
      },
      "source": [
        "# Random test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS3_IVt9z_8G",
        "outputId": "be80feb1-fe12-4c0f-9713-1ea259fba302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- LOADING TRAINING SET FOR TUNING ---\n",
            "loading from: /content/drive/MyDrive/local_proj_new/normal/rec0_20220811_rbtc_0.1s.metadata\n",
            "loading from: /content/drive/MyDrive/local_proj_new/normal/rec2_20220811_rbtc_0.1s.metadata\n",
            "loading from: /content/drive/MyDrive/local_proj_new/normal/rec3_20220811_rbtc_0.1s.metadata\n",
            "loading from: /content/drive/MyDrive/local_proj_new/normal/rec4_20220811_rbtc_0.1s.metadata\n",
            "--- TUNING SCALER AND CORR PARAMETERS ---\n",
            "--- LOADING TRAINING SET ---\n",
            "loading from: /content/drive/MyDrive/local_proj_new/normal/rec0_20220811_rbtc_0.1s.metadata\n",
            "--- PREPROCESSING TRAINING SPLIT 0 WITH OVERLAP 0.0 ---\n",
            "loading from: /content/drive/MyDrive/local_proj_new/normal/rec2_20220811_rbtc_0.1s.metadata\n",
            "--- PREPROCESSING TRAINING SPLIT 1 WITH OVERLAP 0.0 ---\n",
            "loading from: /content/drive/MyDrive/local_proj_new/normal/rec3_20220811_rbtc_0.1s.metadata\n",
            "--- PREPROCESSING TRAINING SPLIT 2 WITH OVERLAP 0.0 ---\n",
            "loading from: /content/drive/MyDrive/local_proj_new/normal/rec4_20220811_rbtc_0.1s.metadata\n",
            "--- PREPROCESSING TRAINING SPLIT 3 WITH OVERLAP 0.0 ---\n",
            "window split train size:\n",
            "(956, 100, 50)\n",
            "window action train size:\n",
            "956\n",
            "--- LOADING TEST SET ---\n",
            "loading from: /content/drive/MyDrive/local_proj_new/collisions/rec1_collision_20220811_rbtc_0.1s.metadata\n",
            "--- PREPROCESSING TEST SPLIT 0 ---\n",
            "loading from: /content/drive/MyDrive/local_proj_new/collisions/rec5_collision_20220811_rbtc_0.1s.metadata\n",
            "--- PREPROCESSING TEST SPLIT 1 ---\n",
            "window split test size:\n",
            "(34077, 100, 50)\n",
            "window action test size:\n",
            "34077\n",
            "timestamps\n",
            "34077\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import tsfel\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import tensorflow as tf\n",
        "import keras_tuner\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
        "\n",
        "ROOT = '/content/drive/MyDrive/MLinApps-Proj-FP01'\n",
        "\n",
        "ROOTDIR_DATASET_NORMAL = ROOT + \"/normal/\"\n",
        "ROOTDIR_DATASET_ANOMALY = ROOT + \"/collisions/\"\n",
        "filepath_csv = [ROOTDIR_DATASET_NORMAL + f\"rec{r}_20220811_rbtc_0.1s.csv\" for r in [0, 2, 3, 4]]\n",
        "filepath_meta = [ROOTDIR_DATASET_NORMAL + f\"rec{r}_20220811_rbtc_0.1s.metadata\" for r in [0, 2, 3, 4]]\n",
        "filepath_csv_anomaly = [ROOTDIR_DATASET_ANOMALY + f\"rec{r}_collision_20220811_rbtc_0.1s.csv\" for r in [1, 5]]\n",
        "filepath_meta_anomaly = [ROOTDIR_DATASET_ANOMALY + f\"rec{r}_collision_20220811_rbtc_0.1s.metadata\" for r in [1, 5]]\n",
        "\n",
        "\n",
        "def get_df_action(filepaths_csv, filepaths_meta, action2int=None, delimiter=\";\"):\n",
        "    # Load dataframes\n",
        "    #print(\"Loading data.\")\n",
        "    # Make dataframes\n",
        "    # Some classes show the output boolean parameter as True rather than true. Fix here\n",
        "    dfs_meta = list()\n",
        "    for filepath in filepaths_meta:\n",
        "        print(f\"loading from: {filepath}\")\n",
        "        df_m = pd.read_csv(filepath, sep=delimiter)\n",
        "        df_m.str_repr = df_m.str_repr.str.replace('True', 'true')\n",
        "        df_m['filepath'] = filepath\n",
        "        dfs_meta.append(df_m)\n",
        "\n",
        "\n",
        "    df_meta = pd.concat(dfs_meta)\n",
        "    df_meta.index = pd.to_datetime(df_meta.init_timestamp.astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "    df_meta['completed_timestamp'] = pd.to_datetime(df_meta.completed_timestamp.astype('datetime64[ms]'),\n",
        "                                                    format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "    df_meta['init_timestamp'] = pd.to_datetime(df_meta.init_timestamp.astype('datetime64[ms]'),\n",
        "                                               format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "\n",
        "    # Eventually reduce number of classes\n",
        "    # df_meta['str_repr'] = df_meta.str_repr.str.split('=', expand = True,n=1)[0]\n",
        "    # df_meta['str_repr'] = df_meta.str_repr.str.split('(', expand=True, n=1)[0]\n",
        "\n",
        "    actions = df_meta.str_repr.unique()\n",
        "    dfs = [pd.read_csv(filepath_csv, sep=\";\") for filepath_csv in filepaths_csv]\n",
        "    df = pd.concat(dfs)\n",
        "\n",
        "    # Sort columns by name !!!\n",
        "    df = df.sort_index(axis=1)\n",
        "\n",
        "    # Set timestamp as index\n",
        "    df.index = pd.to_datetime(df.time.astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "    # Drop useless columns\n",
        "    columns_to_drop = [column for column in df.columns if \"Abb\" in column or \"Temperature\" in column]\n",
        "    df.drop([\"machine_nameKuka Robot_export_active_energy\",\n",
        "             \"machine_nameKuka Robot_import_reactive_energy\"] + columns_to_drop, axis=1, inplace=True)\n",
        "    signals = df.columns\n",
        "\n",
        "    df_action = list()\n",
        "    for action in actions:\n",
        "        for index, row in df_meta[df_meta.str_repr == action].iterrows():\n",
        "            start = row['init_timestamp']\n",
        "            end = row['completed_timestamp']\n",
        "            df_tmp = df.loc[start: end].copy()\n",
        "            df_tmp['action'] = action\n",
        "            # Duration as string (so is not considered a feature)\n",
        "            df_tmp['duration'] = str((row['completed_timestamp'] - row['init_timestamp']).total_seconds())\n",
        "            df_action.append(df_tmp)\n",
        "    df_action = pd.concat(df_action, ignore_index=True)\n",
        "    df_action.index = pd.to_datetime(df_action.time.astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "    df_action = df_action[~df_action.index.duplicated(keep='first')]\n",
        "\n",
        "    # Drop NaN\n",
        "    df = df.dropna(axis=0)\n",
        "    df_action = df_action.dropna(axis=0)\n",
        "\n",
        "    if action2int is None:\n",
        "        action2int = dict()\n",
        "        j = 1\n",
        "        for label in df_action.action.unique():\n",
        "            action2int[label] = j\n",
        "            j += 1\n",
        "\n",
        "    df_merged = df.merge(df_action[['action']], left_index=True, right_index=True, how=\"left\")\n",
        "    # print(f\"df_merged len: {len(df_merged)}\")\n",
        "    # Where df_merged in NaN Kuka is in idle state\n",
        "    df_idle = df_merged[df_merged['action'].isna()].copy()\n",
        "    df_idle['action'] = 'idle'\n",
        "    df_idle['duration'] = df_action.duration.values.astype(float).mean().astype(str)\n",
        "    df_action = pd.concat([df_action, df_idle])\n",
        "\n",
        "    df_action = df_action.sort_index()\n",
        "\n",
        "    # ile label must be 0 for debug mode\n",
        "    action2int['idle'] = 0\n",
        "    #print(f\"Found {len(set(df_action['action']))} different actions.\")\n",
        "    #print(\"Loading data done.\\n\")\n",
        "\n",
        "    return df_action, df, df_meta, action2int\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def sample_as_splitter(signal, window_size, overlap=0):\n",
        "    \"\"\"sample same size as splitter of tsfel\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    signal : nd-array or pandas DataFrame\n",
        "        input signal\n",
        "    window_size : int\n",
        "        number of points of window size\n",
        "    overlap : float\n",
        "        percentage of overlap, value between 0 and 1 (exclusive)\n",
        "        Default: 0\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        list of signal windows\n",
        "    \"\"\"\n",
        "    if not isinstance(window_size, int):\n",
        "        raise SystemExit(\"window_size must be an integer.\")\n",
        "    step = int(round(window_size)) if overlap == 0 else int(round(window_size * (1 - overlap)))\n",
        "    if step == 0:\n",
        "        raise SystemExit(\n",
        "            \"Invalid overlap. \" \"Choose a lower overlap value.\",\n",
        "        )\n",
        "    if len(signal) % window_size == 0 and overlap == 0:\n",
        "        return [signal[i] for i in range(0, len(signal), step)]\n",
        "    else:\n",
        "        return [signal[i] for i in range(0, len(signal) - window_size + 1, step)]\n",
        "\n",
        "\n",
        "def MTGFLOWLOADER_action(opt):\n",
        "\n",
        "    #Use same overlap for test and train set\n",
        "\n",
        "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "    if opt['num_features']==50:\n",
        "        mode = 0\n",
        "    elif opt['num_features']==51:\n",
        "        mode = 1\n",
        "    else:\n",
        "        assert False, \"not implemented\"\n",
        "\n",
        "    frequency = 1/10\n",
        "    duration = opt['window_size']*frequency\n",
        "    overlap = (opt['window_size'] - opt['stride']) / opt['window_size']\n",
        "\n",
        "\n",
        "    print(\"--- LOADING TRAINING SET FOR TUNING ---\")\n",
        "    #NOTE: all action2int unique values must appear in training set\n",
        "    #action2int and action_cols must not be changed at test time\n",
        "    X_train_action, _, _, action2int = get_df_action(filepath_csv, filepath_meta)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    shape = (-1, opt['window_size'], opt['num_features'])\n",
        "\n",
        "    print(\"--- TUNING SCALER AND CORR PARAMETERS ---\")\n",
        "\n",
        "    #int encoding\n",
        "    X_train_action[\"action\"] = X_train_action[\"action\"].map(action2int)\n",
        "\n",
        "    X_train = X_train_action.fillna(0) #1 fill null values\n",
        "\n",
        "    if mode != 1:\n",
        "        X_train = X_train.drop([\"time\", \"duration\", \"action\"], axis=1) #2 drop timestamps columns\n",
        "    else:\n",
        "        X_train = X_train.drop([\"time\", \"duration\"], axis=1) #2 drop timestamps columns\n",
        "\n",
        "    corr_features = tsfel.correlated_features(X_train,threshold=0.95) #action should not be too much correlated\n",
        "\n",
        "    X_train = X_train.drop(corr_features, axis=1) #3 drop corr features\n",
        "\n",
        "    scaler.fit_transform(X_train)\n",
        "\n",
        "    print(\"--- LOADING TRAINING SET ---\")\n",
        "\n",
        "    window_splits_train = []\n",
        "    window_action_train = []\n",
        "\n",
        "    for i in range(4):\n",
        "        #get one csv file at a time, discard incorrect (cross-file) time windows\n",
        "        #--> start from first of at least window_size size)\n",
        "\n",
        "        X_train_action, _, _, _ = get_df_action([filepath_csv[i],], [filepath_meta[i],])\n",
        "\n",
        "        print(f\"--- PREPROCESSING TRAINING SPLIT {i} WITH OVERLAP {overlap} ---\")\n",
        "        #actual preprocessing\n",
        "\n",
        "        #int encoding\n",
        "        X_train_action[\"action\"] = X_train_action[\"action\"].map(action2int)\n",
        "\n",
        "        window_action_train.extend(sample_as_splitter(X_train_action[\"action\"],opt[\"window_size\"], overlap=overlap)) #need ations mapping to rescale by action\n",
        "\n",
        "\n",
        "        X_train = X_train_action.fillna(0) #1 fill null values\n",
        "\n",
        "        if mode != 1:\n",
        "            X_train = X_train.drop([\"time\",\"duration\",\"action\"], axis=1) #2 drop timestamps columns\n",
        "        else:\n",
        "            X_train = X_train.drop([\"time\",\"duration\"], axis=1) #2 drop timestamps columns\n",
        "\n",
        "        X_train = X_train.drop(corr_features, axis=1) #3 drop corr features\n",
        "        X_train = scaler.transform(X_train) #4 normalize\n",
        "\n",
        "\n",
        "        window_splits_train.extend(tsfel.utils.signal_processing.signal_window_splitter(X_train.copy(), opt['window_size'], overlap=overlap))\n",
        "\n",
        "\n",
        "    #output of scaler is np array\n",
        "    window_splits_train = np.concatenate(window_splits_train, axis=0)\n",
        "    window_splits_train = window_splits_train.reshape(shape)\n",
        "\n",
        "    overlap = (opt['window_size'] - 1) / opt['window_size']\n",
        "    #overlap = 0\n",
        "\n",
        "    print(\"window split train size:\")\n",
        "    print(window_splits_train.shape)\n",
        "    print(\"window action train size:\")\n",
        "    print(len(window_action_train))\n",
        "    assert len(window_action_train) == len(window_splits_train) , \"error loading train set actions\"\n",
        "\n",
        "    print(\"--- LOADING TEST SET ---\")\n",
        "\n",
        "    window_splits_test = []\n",
        "    window_action_test = []\n",
        "    window_test_times = pd.DataFrame()\n",
        "\n",
        "    for i in range(2):\n",
        "        #get one csv file at a time, discard incorrect (cross-file) time windows\n",
        "        #--> start from first of at least window_size size)\n",
        "        X_test_action, _, _, _ = get_df_action([filepath_csv_anomaly[i],], [filepath_meta_anomaly[i],])\n",
        "\n",
        "        print(f\"--- PREPROCESSING TEST SPLIT {i} ---\")\n",
        "\n",
        "        df_time = pd.DataFrame()\n",
        "        time = pd.to_datetime(X_test_action.time.astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "        #NOTE: X_test_action returns always timestamp of first file\n",
        "\n",
        "        df_time['start'] = sample_as_splitter(time, opt[\"window_size\"], overlap=overlap)\n",
        "        df_time['end'] = df_time['start'] + pd.to_timedelta(duration, 's')\n",
        "\n",
        "        #int encoding\n",
        "        X_test_action[\"action\"] = X_test_action[\"action\"].map(action2int)\n",
        "        window_action_test.extend(sample_as_splitter(X_test_action[\"action\"],opt[\"window_size\"], overlap=overlap)) #need ations mapping to rescale by action\n",
        "\n",
        "        X_test = X_test_action.fillna(0) #1 fill null values\n",
        "        if mode != 1:\n",
        "            X_test = X_test.drop([\"time\",\"duration\",\"action\"], axis=1)  #2 drop timestamps columns\n",
        "        else:\n",
        "            X_test = X_test.drop([\"time\",\"duration\"], axis=1)\n",
        "\n",
        "        X_test = X_test.drop(corr_features, axis=1) #3 drop corr features\n",
        "        X_test = scaler.transform(X_test) #4 normalize\n",
        "\n",
        "        window_splits_test.extend(tsfel.utils.signal_processing.signal_window_splitter(X_test.copy(), opt['window_size'], overlap=overlap))\n",
        "        #append new timestamps\n",
        "        window_test_times = pd.concat([window_test_times, df_time], axis=0)\n",
        "\n",
        "    #output of scaler is np array\n",
        "    window_splits_test = np.concatenate(window_splits_test, axis=0)\n",
        "    window_splits_test = window_splits_test.reshape(shape)\n",
        "\n",
        "    print(\"window split test size:\")\n",
        "    print(window_splits_test.shape)\n",
        "    print(\"window action test size:\")\n",
        "    print(len(window_action_test))\n",
        "    print(\"timestamps\")\n",
        "    print(len(window_test_times))\n",
        "\n",
        "    assert len(window_test_times) == len(window_splits_test), \"time and data mismatch\"\n",
        "    assert len(window_action_test) == len(window_splits_test) , \"error loading test set actions\"\n",
        "\n",
        "    return window_splits_train, window_action_train, window_splits_test, window_test_times, window_action_test\n",
        "\n",
        "opt = {'num_features':50, 'window_size':100, 'stride':100}\n",
        "\n",
        "_, _, _, test_times, _ = MTGFLOWLOADER_action(opt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtW69jxbzIIn",
        "outputId": "f8208585-0d7d-4249-d50b-1c82adbd58d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test 0\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "test 1\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "test 2\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "test 3\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "test 4\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "test 5\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "test 6\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "test 7\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "test 8\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "test 9\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "test 10\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "test 11\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "test 12\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "test 13\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "test 14\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "test 15\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "test 16\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "test 17\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "test 18\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "test 19\n",
            "--- LOADED 107 COLLISIONS ---\n",
            "f1 mean: 0.21659268377381022, f1 std: 0.0002841619850897175\n",
            "auroc: 0.500171983961916, auroc std: 0.002892736446929721\n",
            "auprc: 0.12115531248081213, auprc std: 0.0014416435919406036\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "ROOTDIR_DATASET_ANOMALY = ROOT + \"/collisions/\"\n",
        "filepath_csv_anomaly = [ROOTDIR_DATASET_ANOMALY + f\"rec{r}_collision_20220811_rbtc_0.1s.csv\" for r in [1, 5]]\n",
        "filepath_meta_anomaly = [ROOTDIR_DATASET_ANOMALY + f\"rec{r}_collision_20220811_rbtc_0.1s.metadata\" for r in [1, 5]]\n",
        "\n",
        "\n",
        "def calculate_auc(x, y):\n",
        "    x=np.array(x)\n",
        "    y=np.array(y)\n",
        "    # Ensure TPR and FPR arrays are sorted together by FPR (ascending)\n",
        "    sorted_idx = np.argsort(x)\n",
        "    x = x[sorted_idx]\n",
        "    y = y[sorted_idx]\n",
        "\n",
        "    # Calculate trapezoidal areas for each interval\n",
        "    auc = np.trapz(y, x)\n",
        "\n",
        "    return auc\n",
        "\n",
        "\n",
        "def convert(times_windowed):\n",
        "\n",
        "    df = pd.DataFrame(columns = ['start','end'])\n",
        "    for time_window in times_windowed: #iterate over dimension 0, extract window timestamps\n",
        "        start = time_window[0][0]\n",
        "        end = time_window[-1][0]\n",
        "        df = pd.concat([df, pd.DataFrame({'start':start,'end':end})], ignore_index=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def compute_average_score(scores, opt):\n",
        "\n",
        "    window_size = opt['window_size']\n",
        "\n",
        "    adjusted_anomalies = np.zeros(len(scores))\n",
        "\n",
        "    for i in range(len(scores)-window_size):\n",
        "        adjusted_anomalies[i:i + window_size] += scores[i]\n",
        "\n",
        "    counts = np.zeros(len(scores))\n",
        "    for i in range(len(scores)-window_size):\n",
        "        counts[i:i + window_size] += 1\n",
        "\n",
        "    adjusted_anomalies = adjusted_anomalies / counts\n",
        "\n",
        "    return adjusted_anomalies\n",
        "\n",
        "\n",
        "def metrics_by_point_vectorized(scores, time_collision, full=False, convert=False):\n",
        "    #score is attributed to last point in window\n",
        "\n",
        "    if full:\n",
        "        thresholds = np.sort(scores)\n",
        "    else:\n",
        "        thresholds = np.linspace(scores.min(), scores.max(), num=300)\n",
        "\n",
        "    if convert:\n",
        "        time_collision = convert(time_collision) #TADGANLOADER conversion\n",
        "\n",
        "    collisions = pd.read_excel(ROOT +\"/collisions/20220811_collisions_timestamp.xlsx\")\n",
        "    collisions_init = collisions[collisions['Inizio/fine'] == \"i\"].Timestamp - pd.to_timedelta([2] * len(collisions[collisions['Inizio/fine'] == \"i\"].Timestamp), 'h')\n",
        "    collision_end = collisions[collisions['Inizio/fine'] == \"f\"].Timestamp - pd.to_timedelta([2] * len(collisions[collisions['Inizio/fine'] == \"f\"].Timestamp), 'h')\n",
        "\n",
        "    time_collision = time_collision[:len(scores)]\n",
        "    assert len(scores) == len(time_collision), \"unmatching score/thresholds/timestamp\"\n",
        "    print(f\"--- LOADED {len(collisions_init)} COLLISIONS ---\")\n",
        "\n",
        "    # Convert timestamps to numpy arrays\n",
        "    start_times = time_collision['start'].to_numpy().astype('datetime64[ns]')\n",
        "    end_times = time_collision['end'].to_numpy().astype('datetime64[ns]')\n",
        "\n",
        "    collisions_init_np = collisions_init.to_numpy().astype('datetime64[ns]')\n",
        "    collisions_end_np = collision_end.to_numpy().astype('datetime64[ns]')\n",
        "\n",
        "    # Create a mask for each threshold\n",
        "    threshold_masks = scores[:, np.newaxis] >= thresholds\n",
        "\n",
        "    n_samples = len(scores)\n",
        "\n",
        "    # Calculate metrics for each threshold\n",
        "    results = []\n",
        "    for threshold_mask in threshold_masks.T:\n",
        "\n",
        "        pos_pred  = np.sum(threshold_mask)\n",
        "        neg_pred = n_samples - pos_pred\n",
        "\n",
        "        # count anomaly timestamps included in an anomaly window -> tp\n",
        "        collision_in_window = ((start_times[threshold_mask] >= collisions_init_np[:, np.newaxis]) & \\\n",
        "                              (start_times[threshold_mask] < collisions_end_np[:, np.newaxis]))\n",
        "\n",
        "        tp = np.sum(collision_in_window) #overall sum is necessary\n",
        "        fp = pos_pred - tp\n",
        "\n",
        "        not_threshold_mask = np.where(threshold_mask, False, True)\n",
        "\n",
        "        # count non anomaly timestamps included in an anomaly window -> fn\n",
        "        false_not_collision_in_window = ((start_times[not_threshold_mask] >= collisions_init_np[:, np.newaxis]) & \\\n",
        "                                        (start_times[not_threshold_mask] < collisions_end_np[:, np.newaxis]))\n",
        "\n",
        "        fn = np.sum(false_not_collision_in_window)\n",
        "        tn = neg_pred - fn\n",
        "\n",
        "        anomaly_indices = np.where(threshold_mask)[0][np.any(collision_in_window, axis=0)]\n",
        "\n",
        "        cm_anomaly = np.array([[tn, fp], [fn, tp]])\n",
        "\n",
        "        precision = tp / (tp + fp) if tp + fp != 0 else 0\n",
        "        recall = tp / (tp + fn) if tp + fn != 0 else 0\n",
        "        fpr = fp / (fp + tn) if fp + tn != 0 else 0\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn) if tp + tn + fp + fn != 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n",
        "\n",
        "        results.append((recall, precision, fpr, accuracy, f1, cm_anomaly, anomaly_indices))\n",
        "\n",
        "    recalls, precisions, fprs, accuracies, f1s, cms, anomaly_indices_list = zip(*results)\n",
        "\n",
        "    return recalls, precisions, fprs, accuracies, f1s, cms, anomaly_indices_list\n",
        "\n",
        "\n",
        "f1_res = []\n",
        "auroc_res = []\n",
        "auprc_res = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f\"test {i}\")\n",
        "    scores = np.random.rand(34077)\n",
        "\n",
        "    recalls, precisions, fprs, _, f1s, _, _ = metrics_by_point_vectorized(scores, test_times, full=True)\n",
        "    f1_res.append(max(f1s))\n",
        "    auroc_res.append(calculate_auc(fprs, recalls))\n",
        "    auprc_res.append(calculate_auc(recalls, precisions))\n",
        "\n",
        "f1 = np.array(f1_res)\n",
        "auroc = np.array(auroc_res)\n",
        "auprc = np.array(auprc_res)\n",
        "\n",
        "print(f\"f1 mean: {np.mean(f1)}, f1 std: {np.std(f1)}\")\n",
        "print(f\"auroc: {np.mean(auroc)}, auroc std: {np.std(auroc)}\")\n",
        "print(f\"auprc: {np.mean(auprc)}, auprc std: {np.std(auprc)}\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "d6oGreBuu4Ao"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}